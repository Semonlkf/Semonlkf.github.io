<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Pandas教程</title>
    <url>/2023/08/15/Pandas%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Tutorials</category>
      </categories>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/08/12/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a class="link"   href="https://hexo.io/" >Hexo <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>! This is your very first post. Check <a class="link"   href="https://hexo.io/docs/" >documentation <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> for more info. If you get any problems when using Hexo, you can find the answer in <a class="link"   href="https://hexo.io/docs/troubleshooting.html" >troubleshooting <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> or you can ask me on <a class="link"   href="https://github.com/hexojs/hexo/issues" >GitHub <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/writing.html" >Writing <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/server.html" >Server <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/generating.html" >Generating <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/one-command-deployment.html" >Deployment <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
]]></content>
  </entry>
  <entry>
    <title>RT系列</title>
    <url>/2024/04/11/RT%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<h2 id="RT-1"><a href="#RT-1" class="headerlink" title="RT-1"></a>RT-1</h2><h3 id="简单概括"><a href="#简单概括" class="headerlink" title="简单概括"></a>简单概括</h3><p>RT-1模型在包含130k个episode的大型真实机器人数据集上进行训练，该数据集涵盖了700 多项任务，使用Everyday Robots (EDR)的13台机器人在17个月内收集而成。数据集中展示的一组高级技能包括拾取和放置物品、打开和关闭抽屉、将物品放入和取出抽屉、将细长的物品直立放置、敲倒物体、拉出餐巾纸和打开罐子。</p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><p>指令+图片</p>
<h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><p>3HZ刷新率的动作集合</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们使用了13个EDR机器人操纵器，每个都带有7个自由度的手臂、一个2指夹持器和一个移动底座，在17个月内收集了13万数据集。我们使用人类通过远程操作提供的演示，并用机器人刚刚执行的指令的文本描述对每一集进行注释。 数据集中表示的一组高级技能包括拾取和放置物品、 打开和关闭抽屉、将物品放入和取出抽屉、将细长的物品直立放置、将物体打翻、拉餐巾和打开罐子。生成的数据集包括130k+集，涵盖使用许多不同对象的700多个任务。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">The two main challenges lie in assembling the right dataset and designing the right model. While data collection and curation is often the &quot;unsung hero&quot; of many large-scale machine learning projects (Radford et al., 2021; Ramesh et al., 2021), this is especially true in robotics, where datasets are often robot-specific and gathered manually (Dasari et al., 2019; Ebert et al., 2021).</span><br></pre></td></tr></table></figure></div>

<p>两个主要挑战在于组装正确的数据集和设计正确的模型。虽然数据收集和管理通常是许多大规模机器学习项目的“非替代英雄”（Radford 等人，2021；Ramesh 等人，2021），但在机器人技术中尤其如此，其中数据集通常是特定于机器人的并手动收集的（Dasari 等人，2019；Ebert 等人，2021）。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">Our goal is to build a system that exhibits high performance, generalization to new tasks, and robustness to distractors and backgrounds. We therefore aim to collect a large, diverse dataset of robot trajectories that includes multiple tasks, objects and environments. Our primary dataset consists of∼130k robot demonstrations, collected with a fleet of 13 robots over the course of 17 months. We conducted this large-scale data collection in a series of office kitchen segments, which we refer to asrobot classrooms, shown in Fig. 2. More details on data collection are in Appendix C.2.</span><br></pre></td></tr></table></figure></div>

<p>我们的目标是构建一个表现出高性能、泛化到新任务以及对干扰物和背景的鲁棒性的系统。因此，我们的目标是收集一个大型、多样化的机器人轨迹数据集，其中包括多个任务、对象和环境。我们的主要数据集由∼130k机器人演示组成，在17个月的时间里用13个机器人车队收集。我们在一系列办公室厨房段进行了这种大规模的数据收集，我们称之为机器人教室，如图2所示。关于数据收集的更多细节见附录C.2。</p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>RT-1建立在一个转换器架构(transformer)上, 该架构从机器人的相机中获取图像的简短历史以及以自然语言表达的任务描述作为输入,并直接输出标记化的动作。<br>RT-1的体系结构类似于针对具有因果掩蔽的标准分类交叉熵目标训练的当代仅解码器序列模型。其主要功能包括:图像标记化、动作标记化和标记压缩,如下所述。<br><strong>图像标记化</strong>:我们通过在ImageNet上预训练的EfficientNet-B3模型传递图像,然后将生成的9x9x512空间特征图扁平化为81个标记。图像分词器以自然语言任务指令条件,并使用初始化为身份的FiLM层在早期提取与任务相关的图像特征。<br><strong>动作标记化</strong>:机器人的动作维度是手臂运动的7个变量(x、y. z. 滚动、俯仰、偏航、夹具打开)，3个基座运动变量(x、 y.偏航)， 以及一个额外的离散变量来切换在三种模式之间:控制臂、控制基地或终止剧集。每个动作维度被离散化为256个bin.<br><strong>令牌压缩</strong>:该模型自适应地选择图像令牌的软组合,这些组合可以根据它们对使用元素注意模块TokenL earner进行学习的影响进行压缩，从而使推理速度提高2.4倍以上。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Semonlkf/image@main/img/rt1.png"
                     
                ></p>
<h2 id="RT-2"><a href="#RT-2" class="headerlink" title="RT-2"></a>RT-2</h2><h3 id="简单概括-1"><a href="#简单概括-1" class="headerlink" title="简单概括"></a>简单概括</h3><p>据了解，谷歌DeepMind这项成果由54位研究员合作产出，前前后后拉扯7个月。<br>视觉-语言模型(vlm)在网络规模的数据集上进行训练,使这些系统在识别视觉或语言模式和跨不同语言操作方面表现得非常好。但是对于机器人来说，要达到类似的能力水平,<br>他们需要收集机器人在每个对象、环境、任务和情况下的第一手数据。<br>本文提出robot Transformer 2 (RT-2)，一种新的视觉-语言动作(VLA)模型，从网络和机器人数据中学习,并将这种知识转换为用于机器人控制的通用指令，同时保留网络规模<br>的能力。<br>RT-2显显出改进的泛化能力以及语义和视觉理解，超出了它接触的机器人数据。这包括解释新命令并通过执行基本推理(例如关于对象类别或高级描述的推理)来响应用户命令。</p>
<h3 id="输入-1"><a href="#输入-1" class="headerlink" title="输入"></a>输入</h3><p>图片+文本</p>
<h3 id="输出-1"><a href="#输出-1" class="headerlink" title="输出"></a>输出</h3><p>动作（编码为文本）<br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Semonlkf/image@main/img/rt2.png"
                     
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Semonlkf/image@main/img/rt3.png"
                     
                ></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">1 continue</span><br></pre></td></tr></table></figure></div>

<h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><ul>
<li>backbone 是50亿和550亿的PaLl-X、30 亿的PaLl以及120亿的PaLM-E。</li>
<li>为了提升大模型本身的能力,把思维链、向量数据库和无梯度架构(no-gradient-architectures) 都用上了。</li>
<li>将原本非常具体的机器人动作数据，转变成文本token。 视觉-语言-行动” (VLA)模型最大的模型.55B参数RT-2-PaLI-X-55B模型，可以运行在1- 3hz的频率。该模型的较小版本包含5B个参数，可以以大约5赫兹的频率运行。</li>
<li>模型微调这里还没太懂<br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Semonlkf/image@main/img/rt5.png"
                     
                ></li>
</ul>
<h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><p>在一些任务上，有了很大的进步。<br>但泛化性一般，抓东西效果可能不是很好<br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://cdn.jsdelivr.net/gh/Semonlkf/image@main/img/20240319153415.png"
                     
                ></p>
<h2 id="RT-H"><a href="#RT-H" class="headerlink" title="RT-H"></a>RT-H</h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><ul>
<li>讨论了由于任务的语义多样性，直接从高层任务映射到机器人动作的限制，提出使用语言描述低层动作（语言动作）作为中间步骤来增强任务间的数据共享并允许人为纠正。</li>
<li><strong>核心思想：</strong> RT-H引入了一个使用语言动作的行为层级，其中机器人首先基于给定的任务和观察学习预测语言动作，然后在这些语言动作的条件下预测动作。这种方法利用了语言的结构来更好地理解和执行任务，使策略更加健壮和灵活，并从人提供的语言动作校正中受益。</li>
</ul>
<h3 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h3><p>应该还是RT-2的数据集，加入Diverse数据集</p>
<h3 id="输入-2"><a href="#输入-2" class="headerlink" title="输入"></a>输入</h3><p>自然语言的高层任务描述和场景的视觉观察。</p>
<h3 id="输出-2"><a href="#输出-2" class="headerlink" title="输出"></a>输出</h3><p>两层：<br>低级的动作语言<br>基于这些语言动作和原始输入的机器人动作</p>
]]></content>
      <categories>
        <category>Emboied AI</category>
      </categories>
  </entry>
</search>
